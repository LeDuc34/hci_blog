<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  LAB Implementing ML Agents in Unity 3D · victormasseboeuf
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Victor Masseboeuf">
<meta name="description" content="
  Building a Game AI with Unity ML-Agents: A Pong Post-Mortem
  
    
    Link to heading
  



  Introduction
  
    
    Link to heading
  

Reinforcement learning in video games has fascinated developers since DeepMind demonstrated that AI could surpass humans at Atari games. Today, Unity ML-Agents makes this technology accessible to developers, allowing the creation of intelligent opponents without being a machine learning expert. In this article, I share my experience developing a Pong game with a trained AI, the challenges encountered, and the solutions found.">
<meta name="keywords" content="blog,developer,personal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="LAB Implementing ML Agents in Unity 3D">
  <meta name="twitter:description" content="Building a Game AI with Unity ML-Agents: A Pong Post-MortemLink to headingIntroductionLink to headingReinforcement learning in video games has fascinated developers since DeepMind demonstrated that AI could surpass humans at Atari games. Today, Unity ML-Agents makes this technology accessible to developers, allowing the creation of intelligent opponents without being a machine learning expert. In this article, I share my experience developing a Pong game with a trained AI, the challenges encountered, and the solutions found.">

<meta property="og:url" content="https://leduc34.github.io/hci_blog/projects/ml-agents/">
  <meta property="og:site_name" content="victormasseboeuf">
  <meta property="og:title" content="LAB Implementing ML Agents in Unity 3D">
  <meta property="og:description" content="Building a Game AI with Unity ML-Agents: A Pong Post-MortemLink to headingIntroductionLink to headingReinforcement learning in video games has fascinated developers since DeepMind demonstrated that AI could surpass humans at Atari games. Today, Unity ML-Agents makes this technology accessible to developers, allowing the creation of intelligent opponents without being a machine learning expert. In this article, I share my experience developing a Pong game with a trained AI, the challenges encountered, and the solutions found.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:published_time" content="2025-11-19T15:16:04+02:00">
    <meta property="article:modified_time" content="2025-11-19T15:16:04+02:00">




<link rel="canonical" href="https://leduc34.github.io/hci_blog/projects/ml-agents/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/hci_blog/css/coder.min.022594d625780e2edf64581b893d32cb35c11de5d88953ea4ad3c2e45451e214.css" integrity="sha256-AiWU1iV4Di7fZFgbiT0yyzXBHeXYiVPqStPC5FRR4hQ=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/hci_blog/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://leduc34.github.io/hci_blog/">
      victormasseboeuf
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/hci_blog/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/hci_blog/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/hci_blog/projects/">Projects</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container page">
  <article>
    <header>
      <h1 class="title">
        <a class="title-link" href="https://leduc34.github.io/hci_blog/projects/ml-agents/">
          LAB Implementing ML Agents in Unity 3D
        </a>
      </h1>
    </header>

    <h1 id="building-a-game-ai-with-unity-ml-agents-a-pong-post-mortem">
  Building a Game AI with Unity ML-Agents: A Pong Post-Mortem
  <a class="heading-link" href="#building-a-game-ai-with-unity-ml-agents-a-pong-post-mortem">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p><img src="/hci_blog/images/pong.png" alt=""></p>
<h2 id="introduction">
  Introduction
  <a class="heading-link" href="#introduction">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Reinforcement learning in video games has fascinated developers since DeepMind demonstrated that AI could surpass humans at Atari games. Today, Unity ML-Agents makes this technology accessible to developers, allowing the creation of intelligent opponents without being a machine learning expert. In this article, I share my experience developing a Pong game with a trained AI, the challenges encountered, and the solutions found.</p>
<h2 id="what-is-unity-ml-agents">
  What is Unity ML-Agents?
  <a class="heading-link" href="#what-is-unity-ml-agents">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Unity ML-Agents is an open-source toolkit that transforms the Unity editor into a training environment for reinforcement learning. The principle is elegant: your Unity game becomes a simulator where virtual agents learn through trial and error, exactly like a human player would progressively discover the game mechanics.</p>
<p>The architecture relies on three main components. The agent perceives its environment through observations (positions, velocities, distances), makes decisions in the form of actions (move, jump, shoot), and receives rewards that guide its learning. The agent&rsquo;s brain can be controlled in three ways: heuristically for debugging, by a human for demonstration, or by a trained neural network.</p>
<h2 id="setting-up-ml-agents-the-fundamentals">
  Setting Up ML-Agents: The Fundamentals
  <a class="heading-link" href="#setting-up-ml-agents-the-fundamentals">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="installation-and-configuration">
  Installation and Configuration
  <a class="heading-link" href="#installation-and-configuration">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The first step involves installing the ML-Agents package in Unity via the Package Manager. Then you need to install Python and the <code>mlagents</code> package via pip, which contains the training algorithms (PPO, SAC). This part may seem simple in theory but comes with its share of surprises in practice.</p>
<p>The first pitfall concerns dependencies. ML-Agents relies on PyTorch, which itself may require CUDA for GPU acceleration. Versions must be carefully aligned: a GPU that&rsquo;s too recent may not be supported by the available CUDA versions, forcing you to fall back to CPU training. In my case, my graphics card was too new for the CUDA versions supported by ML-Agents, making GPU acceleration impossible. This significantly slowed down the training process, a constraint I had to work around throughout the project.</p>
<h3 id="creating-your-first-agent">
  Creating Your First Agent
  <a class="heading-link" href="#creating-your-first-agent">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>To implement an agent, you need to inherit from the <code>Agent</code> class and override three key methods:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-csharp" data-lang="csharp"><span class="line"><span class="cl"><span class="kd">public</span> <span class="k">class</span> <span class="nc">PongAgent</span> <span class="p">:</span> <span class="n">Agent</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kd">public</span> <span class="kd">override</span> <span class="k">void</span> <span class="n">CollectObservations</span><span class="p">(</span><span class="n">VectorSensor</span> <span class="n">sensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Add observations about the environment</span>
</span></span><span class="line"><span class="cl">        <span class="n">sensor</span><span class="p">.</span><span class="n">AddObservation</span><span class="p">(</span><span class="n">transform</span><span class="p">.</span><span class="n">localPosition</span><span class="p">.</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">sensor</span><span class="p">.</span><span class="n">AddObservation</span><span class="p">(</span><span class="n">ball</span><span class="p">.</span><span class="n">transform</span><span class="p">.</span><span class="n">position</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">sensor</span><span class="p">.</span><span class="n">AddObservation</span><span class="p">(</span><span class="n">ball</span><span class="p">.</span><span class="n">velocity</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kd">public</span> <span class="kd">override</span> <span class="k">void</span> <span class="n">OnActionReceived</span><span class="p">(</span><span class="n">ActionBuffers</span> <span class="n">actions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Execute the chosen action</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">moveX</span> <span class="p">=</span> <span class="n">actions</span><span class="p">.</span><span class="n">DiscreteActions</span><span class="p">[</span><span class="m">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Move the paddle based on the action</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kd">public</span> <span class="kd">override</span> <span class="k">void</span> <span class="n">OnEpisodeBegin</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Reset the environment for a new episode</span>
</span></span><span class="line"><span class="cl">        <span class="n">ResetBallPosition</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="n">ResetScore</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>The reward system is crucial: it&rsquo;s what teaches the agent to distinguish good from bad behavior. For Pong, I initially thought of simple rewards: +1 for scoring a point, -1 for conceding one. But reality proved more nuanced.</p>
<h2 id="the-pong-project-from-ambition-to-reality">
  The Pong Project: From Ambition to Reality
  <a class="heading-link" href="#the-pong-project-from-ambition-to-reality">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="initial-architecture">
  Initial Architecture
  <a class="heading-link" href="#initial-architecture">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>My Pong project is built around four main scripts:</p>
<p><strong>BallController</strong>: Manages ball behavior, ensuring it stays on the ground plane with constant velocity. I opted for Unity&rsquo;s physics engine rather than manual trajectory calculations, which proved to be the right choice.</p>
<p><strong>PongAgent</strong>: Controls the AI paddle and manages agent training. This is where all the ML-Agents logic resides: observations, actions, and rewards.</p>
<p><strong>GameManager</strong>: Orchestrates the overall game logic, managing episodes, scores, and game state transitions.</p>
<p><strong>UIManager</strong>: Handles the user interface and interactions, providing a clean separation of concerns between game logic and presentation.</p>
<h3 id="challenge-1-the-overpowered-opponent">
  Challenge #1: The Overpowered Opponent
  <a class="heading-link" href="#challenge-1-the-overpowered-opponent">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The biggest challenge wasn&rsquo;t technical but conceptual: how do you design an effective training environment?</p>
<p>My first approach was to pit the learning agent against a simple heuristic opponent that just followed the ball. This seemed reasonable—after all, it&rsquo;s how a beginner would learn. But I quickly discovered a fundamental principle of reinforcement learning: <strong>an opponent that&rsquo;s too strong prevents exploration</strong>.</p>
<p>The agent needs to occasionally win to discover good strategies. Facing a perfect opponent, it never experienced the reward of scoring a point, so it couldn&rsquo;t understand which behaviors led to success. Training stagnated completely.</p>
<p>I then tested several approaches:</p>
<p><strong>Self-Play</strong>: Two agents competing against each other. The theory was appealing—they would improve together. Reality: chaotic learning and sub-optimal behaviors. Without a reference point, the agents developed peculiar strategies that worked against each other but weren&rsquo;t generalisable.</p>
<p><strong>Agent vs Heuristic</strong>: Going back to the heuristic opponent, but accepting it was too strong. Result: the agent remained stuck and learned nothing. This confirmed that the difficulty was the core issue, not the training method.</p>
<p><strong>Curriculum Learning</strong>: Progressive learning in phases. Phase 1 aimed to catch the ball, Phase 2 to score points. This was the most promising approach—I saw visible progress. Despite the CPU-only training time being challenging, I persisted with longer training sessions, and it eventually paid off. The agent successfully learned to play competitively.</p>
<h3 id="challenge-2-physics-management">
  Challenge #2: Physics Management
  <a class="heading-link" href="#challenge-2-physics-management">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Initially, I tried to manually manage collisions and ball trajectories. This seemed like a good idea to maintain full control. The reality: imprecise bounce angle calculations, the ball getting stuck on walls, and edge cases multiplying.</p>
<p>The solution was to embrace Unity&rsquo;s physics engine:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-csharp" data-lang="csharp"><span class="line"><span class="cl"><span class="c1">// Ball setup with physics</span>
</span></span><span class="line"><span class="cl"><span class="n">Rigidbody</span> <span class="n">rb</span> <span class="p">=</span> <span class="n">ball</span><span class="p">.</span><span class="n">AddComponent</span><span class="p">&lt;</span><span class="n">Rigidbody</span><span class="p">&gt;();</span>
</span></span><span class="line"><span class="cl"><span class="n">rb</span><span class="p">.</span><span class="n">collisionDetectionMode</span> <span class="p">=</span> <span class="n">CollisionDetectionMode</span><span class="p">.</span><span class="n">Continuous</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">rb</span><span class="p">.</span><span class="n">constraints</span> <span class="p">=</span> <span class="n">RigidbodyConstraints</span><span class="p">.</span><span class="n">FreezePositionY</span><span class="p">;</span> <span class="c1">// Keep ball on ground</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">PhysicMaterial</span> <span class="n">physicsMat</span> <span class="p">=</span> <span class="k">new</span> <span class="n">PhysicMaterial</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="n">physicsMat</span><span class="p">.</span><span class="n">bounciness</span> <span class="p">=</span> <span class="m">1.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">physicsMat</span><span class="p">.</span><span class="n">friction</span> <span class="p">=</span> <span class="m">0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ball</span><span class="p">.</span><span class="n">GetComponent</span><span class="p">&lt;</span><span class="n">Collider</span><span class="p">&gt;().</span><span class="n">material</span> <span class="p">=</span> <span class="n">physicsMat</span><span class="p">;</span>
</span></span></code></pre></div><p>Result: simpler code, precise collisions, realistic physics. This illustrates an important lesson—sometimes the best solution is to use existing, well-tested tools rather than reinventing the wheel.</p>
<h3 id="the-breakthrough">
  The Breakthrough
  <a class="heading-link" href="#the-breakthrough">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>After numerous training iterations and patience with CPU-only training, the ML-Agents approach finally succeeded. The key was accepting the longer training times and carefully tuning the reward structure in the curriculum learning phases.</p>
<p>The trained agent learned not just to react to the ball, but to anticipate trajectories and position itself strategically. While I kept a heuristic fallback in the codebase for testing purposes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-csharp" data-lang="csharp"><span class="line"><span class="cl"><span class="kt">float</span> <span class="n">diffX</span> <span class="p">=</span> <span class="n">ball</span><span class="p">.</span><span class="n">position</span><span class="p">.</span><span class="n">x</span> <span class="p">-</span> <span class="n">paddle</span><span class="p">.</span><span class="n">position</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="n">threshold</span> <span class="p">=</span> <span class="m">0.5f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">diffX</span> <span class="p">&lt;</span> <span class="p">-</span><span class="n">threshold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">discreteActions</span><span class="p">[</span><span class="m">0</span><span class="p">]</span> <span class="p">=</span> <span class="m">1</span><span class="p">;</span> <span class="c1">// Move left</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">diffX</span> <span class="p">&gt;</span> <span class="n">threshold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">discreteActions</span><span class="p">[</span><span class="m">0</span><span class="p">]</span> <span class="p">=</span> <span class="m">2</span><span class="p">;</span> <span class="c1">// Move right</span>
</span></span></code></pre></div><p>The neural network-based agent became the primary opponent, demonstrating more nuanced gameplay than the simple ball-tracking heuristic. The trained model weights are included in the project repository, ready to challenge players.</p>
<h2 id="key-lessons-learned">
  Key Lessons Learned
  <a class="heading-link" href="#key-lessons-learned">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="1-hardware-matters">
  1. Hardware Matters
  <a class="heading-link" href="#1-hardware-matters">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Machine learning is computationally intensive. GPU incompatibility wasn&rsquo;t just an inconvenience—it fundamentally changed what was achievable within the project timeline. Always verify your hardware compatibility before committing to an ML approach.</p>
<h3 id="2-problem-design-before-implementation">
  2. Problem Design Before Implementation
  <a class="heading-link" href="#2-problem-design-before-implementation">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The biggest roadblock wasn&rsquo;t coding but understanding how to structure the learning problem. An overpowered opponent, incorrect reward structure, or poorly designed observations can completely derail training. Analyze the problem thoroughly before diving in headfirst.</p>
<h3 id="3-embrace-incremental-development">
  3. Embrace Incremental Development
  <a class="heading-link" href="#3-embrace-incremental-development">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Curriculum learning showed promise because it breaks down a complex problem into manageable sub-problems. This principle applies beyond ML—decompose challenges into smaller, achievable milestones.</p>
<h3 id="4-perseverance-pays-off">
  4. Perseverance Pays Off
  <a class="heading-link" href="#4-perseverance-pays-off">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>While constraints require pragmatism, sometimes persistence through challenges yields the best results. Despite CPU-only training being slow and frustrating, continuing with longer training sessions ultimately produced a working AI. The ML-Agents infrastructure not only remains in the codebase—it powers the game&rsquo;s core opponent.</p>
<h3 id="5-leverage-existing-tools">
  5. Leverage Existing Tools
  <a class="heading-link" href="#5-leverage-existing-tools">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The physics engine saga illustrates a crucial point: specialized tools exist for a reason. Unity&rsquo;s physics engine handles countless edge cases I would have spent weeks debugging. Know when to build and when to reuse.</p>
<h2 id="the-final-product">
  The Final Product
  <a class="heading-link" href="#the-final-product">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>After overcoming the challenges, the project resulted in a functional, enjoyable Pong game featuring:</p>
<ul>
<li>A futuristic stadium under a starry sky for immersion</li>
<li>A trained ML-Agents AI opponent that demonstrates strategic gameplay</li>
<li>Real-time score display and game timer</li>
<li>Pause system and post-game restart option</li>
<li>Clean, responsive UI</li>
</ul>
<p>Players compete to be first to 7 points against a neural network-trained opponent capable of anticipating trajectories and positioning strategically, creating an engaging and challenging experience that goes beyond simple ball-tracking.</p>
<h2 id="future-improvements">
  Future Improvements
  <a class="heading-link" href="#future-improvements">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>With the successful ML-Agents implementation, several enhancements could further improve the experience:</p>
<p><strong>GPU-Accelerated Retraining</strong>: With GPU access, training time could be dramatically reduced, enabling more experimentation with hyperparameters and reward structures to create an even more sophisticated AI.</p>
<p><strong>Difficulty Levels</strong>: Training multiple agents with different skill levels—from beginner-friendly to expert-level—would broaden the game&rsquo;s appeal and provide progressive challenges.</p>
<p><strong>Enhanced Observations</strong>: Adding more contextual information like opponent paddle position, recent trajectory history, and game momentum could enable even more nuanced strategies.</p>
<p><strong>Opponent Behavior Variation</strong>: Training agents with different playstyles (aggressive vs defensive, reactive vs predictive) would increase replayability and create distinct opponent personalities.</p>
<h2 id="conclusion">
  Conclusion
  <a class="heading-link" href="#conclusion">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Unity ML-Agents opens fascinating possibilities for game AI, and with patience and persistence, these possibilities become reality. Success requires understanding both the technical framework and the learning problem you&rsquo;re trying to solve. Hardware constraints are real and training takes time, but neither are insurmountable obstacles.</p>
<p>The key takeaway? Analyze problems before implementation, adapt your approach when needed, and don&rsquo;t give up when progress seems slow. CPU-only training is challenging but viable. Sometimes the solution requires not changing your approach, but giving it enough time to work.</p>
<p>The complete project, including trained model weights, is available on <a href="https://github.com/LeDuc34/Pong"  class="external-link" target="_blank" rel="noopener">GitHub</a> for those interested in exploring ML-Agents implementation.</p>
<hr>
<p><em>Have you worked with Unity ML-Agents? What challenges did you encounter? Share your experiences in the comments!</em></p>

  </article>
</section>

  

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2019 -
    
    2025
     Victor Masseboeuf 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/hci_blog/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
